# -*- coding: utf-8 -*-
"""MedicalInsuranceClaim Dataset

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_7qgnfxjn-BjZUd5pOQsas0kSS1fho-i

# Import Libraries & Dataset
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("thedevastator/insurance-claim-analysis-demographic-and-health")

print("Path to dataset files:", path)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.preprocessing import LabelEncoder, RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

df=pd.read_csv(os.path.join(path,'insurance_data.csv'))

"""# 2.0 Dataset

2.2 Statistics
"""

df.shape

df.head()

df.describe()

df.info()

plt.hist(df['diabetic'], bins=20)
plt.xlabel('Diabetic distribution')
plt.ylabel('Count')
plt.title('Diabetic')
plt.show()

sns.boxplot(x='smoker', y='claim', data=df)
plt.title('Claim Amount by Smoking Status')
plt.xlabel('Smoker')
plt.ylabel('Claim Amount')
plt.show()

feature_x = 'bloodpressure'
feature_y = 'claim'

plt.figure(figsize=(10, 6))
sns.scatterplot(x=df[feature_x], y=df[feature_y], hue=df[feature_x], palette='viridis')
plt.title(f'{feature_x} vs {feature_y} ')
plt.xlabel(feature_x)
plt.ylabel(feature_y)
plt.legend(title=feature_x)
plt.show()

feature_x = 'bmi'
feature_y = 'bloodpressure'

plt.figure(figsize=(10, 6))
sns.scatterplot(x=df[feature_x], y=df[feature_y], hue=df[feature_x], palette='viridis')
plt.title(f'{feature_x} vs {feature_y} ')
plt.xlabel(feature_x)
plt.ylabel(feature_y)
plt.legend(title=feature_x)
plt.show()

color = sns.color_palette("Set1")
ax = sns.countplot(data = df, x = 'gender', hue = 'smoker', palette = color)
plt.xlabel('gender')
plt.ylabel('smoker')
plt.title('Smoker distribution based on gender')

for container in ax.containers:
    ax.bar_label(container)

plt.show()

"""# 3.0 Preprocessing

Drop Uninformative Lines
"""

df = df.drop(['index', 'PatientID'], axis=1, errors='ignore')

"""## 3.2.1 Handling Missing Values"""

# Step 1: Make a copy before imputation
df_before_impute = df.copy()

# Step 2: Store missing values before
missing_before = df.isnull().sum().to_frame(name='Missing Values Before')

# Step 3: Impute missing values with median (numerical) and mode (categorical)
for col in df.columns:
    if df[col].isnull().sum() > 0:
        if df[col].dtype == 'object':
            mode_val = df[col].mode()[0]
            df[col] = df[col].fillna(mode_val)
        else:
            median_val = df[col].median()
            df[col] = df[col].fillna(median_val)

# Step 4: Store missing values after
missing_after = df.isnull().sum().to_frame(name='Missing Values After')
missing_summary = pd.concat([missing_before, missing_after], axis=1)

print("Missing Values Summary:\n", missing_summary)

# Step 5: Show rows where values were imputed
print("\nBefore Imputation:")
imputed_rows = df_before_impute[df_before_impute.isnull().any(axis=1)]
display(imputed_rows)

print("\nUpdated Rows After Imputation:")
display(df.loc[imputed_rows.index])

"""## 3.2.2 Handling Duplicates"""

duplicate_count = df.duplicated().sum()
print(f"Number of duplicate rows before removal: {duplicate_count}")

if duplicate_count > 0:
  print("\nDuplicate Rows Preview:")
  display(df[df.duplicated()])

# Drop duplicates
df.drop_duplicates(inplace=True)

duplicate_count_after = df.duplicated().sum()
print(f"Number of duplicate rows after removal: {duplicate_count_after}")

print("\nShape of dataset after duplicate removal:", df.shape)

"""## 3.2.3 Categorical Standardization"""

# Store original columns for printing
original_cols = ['smoker', 'gender', 'diabetic']

# Create a copy of the DataFrame to preserve original values
df_original = df.copy()

# Display the original values before standardization
print("Values Before Standardization:")
display(df_original[original_cols].head()) # Display the first few rows

# Standardize Categorical Text in the original DataFrame
for col in df.select_dtypes(include='object').columns:
    if col in ['smoker', 'gender', 'diabetic']:  # Apply to specific columns
        df[col] = df[col].str.lower().str.strip() # Convert all strings to lowercase & remove leading/trailing spaces


# Display the values after standardization
print("\nValues After Standardization:")
display(df[original_cols].head()) # Display the first few rows after standardization

"""## Handling Outliers (REMOVED)

#Visualize histogram for outliers can be removed later
numeric_columns = df.select_dtypes(include=np.number).columns

# Plot histograms
for col in numeric_columns:
    plt.figure(figsize=(6,4))
    plt.hist(df[col], bins=30, color='skyblue', edgecolor='black')
    plt.title(f'Histogram of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()

# Define the IQR outlier removal function
def remove_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return data[(data[column] >= lower) & (data[column] <= upper)]

# Show shape and descriptive stats BEFORE outlier removal
print("Shape before outlier removal:", df.shape)
print("\nSummary statistics before outlier removal:")
display(df.describe())

# Store the original row count
original_rows = df.shape[0]

# Apply IQR outlier removal on all numeric columns

#numeric_columns = df.select_dtypes(include=np.number).columns.drop('claim')
# Note: The outliers in claim (or bmi and blood pressure) may not be considered as outliers, u can try using the line above and check out the model performances. (Rmb to comment the line below)
# I've also tried not removing outliers at all and the model performances are much better.

numeric_columns = df.select_dtypes(include=np.number)
for col in numeric_columns:
    before = df.shape[0]
    df = remove_outliers_iqr(df, col)
    after = df.shape[0]
    print(f"Removed {before - after} outliers from '{col}'")

# Show shape and descriptive stats AFTER outlier removal
print("\nShape after outlier removal:", df.shape)
print("\nSummary statistics after outlier removal:")
display(df.describe())

# Total number of rows removed
rows_removed = original_rows - df.shape[0]
print(f"\nTotal rows removed due to outliers: {rows_removed}")

#Visualize histogram after removing outliers can be removed later
numeric_columns = df.select_dtypes(include=np.number).columns

# Plot histograms
for col in numeric_columns:
    plt.figure(figsize=(6,4))
    plt.hist(df[col], bins=30, color='skyblue', edgecolor='black')
    plt.title(f'Histogram of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()

## 3.2.4 Feature Encoding
"""

# Columns to encode
label_encode_cols = ['gender', 'diabetic', 'smoker']
one_hot_encode_cols = ['region']

# Display original values before encoding
df_original = df.copy()
print("Before Encoding:")
display(df_original.head())

# Label Encode selected binary/ordinal columns
for col in label_encode_cols:
    if col in df.columns:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])

# One-Hot Encode selected nominal columns
df = pd.get_dummies(df, columns=one_hot_encode_cols, prefix='region', drop_first=True, dtype=int)

# Display values after encoding
print("\nAfter Encoding:")
display(df.head())

"""## 3.2.5 Feature Scaling"""

# Select numerical columns
numerical_columns = ['age', 'bmi', 'bloodpressure', 'claim']

original_numerical_robust = df[numerical_columns].copy()

# Apply Robust Scaling
robust_scaler = RobustScaler()
df[numerical_columns] = robust_scaler.fit_transform(df[numerical_columns])

# Display values before and after Robust Scaling
print("Before Robust Scaling:")
display(original_numerical_robust.head())

print("\nAfter Robust Scaling:")
display(df[numerical_columns].head())

"""## 3.2.6 Previewed Dataset"""

print("Final Dataset Shape:", df.shape)
print("\nFinal Dataset Preview:")
df.head()

"""# 4.0 Techniques Used (Initialize + TrainTestSplit)

### 4.1 Linear Regression

4.1.1 Initializing
"""

# Initialize Linear Regression model
linear_model = LinearRegression()

"""4.1.2 Train-test"""

# Define features and target
X = df.drop("claim", axis=1)
y = df["claim"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
linear_model.fit(X_train, y_train)

"""### 4.2 Decision Tree

4.2.1 Initialize
"""

# Initialize Decision Tree Regressor
tree_model = DecisionTreeRegressor(random_state=42)

"""4.2.2 Train-test"""

# Train Decision Tree model
tree_model.fit(X_train, y_train)

"""### 4.3 Random Forest

4.3.1 Initialize
"""

# Initialize Random Forest Regressor
forest_model = RandomForestRegressor(n_estimators=100, random_state=42)

"""4.3.2 Train-test"""

# Train Random Forest model
forest_model.fit(X_train, y_train)

"""### 4.4 K-Nearest Neighbors

4.4.1 Initialize
"""

# Initialize K-Nearest Neighbors Regressor
knn_model = KNeighborsRegressor(n_neighbors=5)

"""4.4.2 Train-test"""

# Train K-Nearest Neighbors model
knn_model.fit(X_train, y_train)

"""# 5.0 Model Validation (kfold cross validation + performance metrics)

## 5.1 Model Evaluation

### 5.1.1 Linear Regression
"""

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Generate predictions using the trained linear regression model
y_pred_lr = linear_model.predict(X_test)

# Calculate metrics
lr_r2 = r2_score(y_test, y_pred_lr) * 100
lr_mse = mean_squared_error(y_test, y_pred_lr)
lr_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lr))
lr_mae = mean_absolute_error(y_test, y_pred_lr)

# Print results
print("Linear Regression Model Evaluation:")
print(f"R-squared (R²): {lr_r2:.2f}%")
print(f"Mean Squared Error (MSE): {lr_mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {lr_rmse:.4f}")
print(f"Mean Absolute Error (MAE): {lr_mae:.4f}")

"""### 5.1.2 Decision Tree"""

# Make predictions first
y_pred_dt = tree_model.predict(X_test)

# Evaluation metrics
dt_r2 = r2_score(y_test, y_pred_dt) * 100
dt_mse = mean_squared_error(y_test, y_pred_dt)
dt_rmse = np.sqrt(dt_mse)
dt_mae = mean_absolute_error(y_test, y_pred_dt)

# Print results
print("Decision Tree Model Evaluation:")
print(f"R-squared (R²): {dt_r2:.2f}%")
print(f"Mean Squared Error (MSE): {dt_mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {dt_rmse:.4f}")
print(f"Mean Absolute Error (MAE): {dt_mae:.4f}")

"""### 5.1.3 Random Forest"""

# Predict first
y_pred_rf = forest_model.predict(X_test)

# Evaluation
rf_r2 = r2_score(y_test, y_pred_rf) * 100
rf_mse = mean_squared_error(y_test, y_pred_rf)
rf_rmse = np.sqrt(rf_mse)
rf_mae = mean_absolute_error(y_test, y_pred_rf)

# Print results
print("Random Forest Model Evaluation:")
print(f"R-squared (R²): {rf_r2:.2f}%")
print(f"Mean Squared Error (MSE): {rf_mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rf_rmse:.4f}")
print(f"Mean Absolute Error (MAE): {rf_mae:.4f}")

"""### 5.1.4 K-Nearest Neighbors"""

# Predict first
y_pred_knn = knn_model.predict(X_test)

# Evaluation
knn_r2 = r2_score(y_test, y_pred_knn) * 100
knn_mse = mean_squared_error(y_test, y_pred_knn)
knn_rmse = np.sqrt(knn_mse)
knn_mae = mean_absolute_error(y_test, y_pred_knn)

# Print results
print("K-Nearest Neighbors Model Evaluation:")
print(f"R-squared (R²): {knn_r2:.2f}%")
print(f"Mean Squared Error (MSE): {knn_mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {knn_rmse:.4f}")
print(f"Mean Absolute Error (MAE): {knn_mae:.4f}")

# Scatterplot for Actual VS Predicted Claim Amount
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# Linear Regression
sns.scatterplot(x=y_test, y=y_pred_lr, color='blue', ax=axes[0, 0])
axes[0, 0].plot([y.min(), y.max()], [y.min(), y.max()], 'r--')
axes[0, 0].set_title('Actual vs Predicted (Linear Regression)')
axes[0, 0].set_xlabel('Actual Claim Amount')
axes[0, 0].set_ylabel('Predicted Claim Amount')
axes[0, 0].grid(True)

# Decision Tree
sns.scatterplot(x=y_test, y=y_pred_dt, color='green', ax=axes[0, 1])
axes[0, 1].plot([y.min(), y.max()], [y.min(), y.max()], 'r--')
axes[0, 1].set_title('Actual vs Predicted (Decision Tree)')
axes[0, 1].set_xlabel('Actual Claim Amount')
axes[0, 1].set_ylabel('Predicted Claim Amount')
axes[0, 1].grid(True)

# Random Forest
sns.scatterplot(x=y_test, y=y_pred_rf, color='purple', ax=axes[1, 0])
axes[1, 0].plot([y.min(), y.max()], [y.min(), y.max()], 'r--')
axes[1, 0].set_title('Actual vs Predicted (Random Forest)')
axes[1, 0].set_xlabel('Actual Claim Amount')
axes[1, 0].set_ylabel('Predicted Claim Amount')
axes[1, 0].grid(True)

# K-Nearest Neighbors
sns.scatterplot(x=y_test, y=y_pred_knn, color='orange', ax=axes[1, 1])
axes[1, 1].plot([y.min(), y.max()], [y.min(), y.max()], 'r--')
axes[1, 1].set_title('Actual vs Predicted (K-Nearest Neighbors)')
axes[1, 1].set_xlabel('Actual Claim Amount')
axes[1, 1].set_ylabel('Predicted Claim Amount')
axes[1, 1].grid(True)

# Adjust layout
plt.tight_layout()
plt.show()

"""## 5.2 Cross Validation

### 5.2.1 Linear Regression Model
"""

from sklearn.model_selection import KFold, cross_validate
import numpy as np

# Define cross-validation strategy
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Define scoring metrics
scoring = {
    'r2': 'r2',
    'mse': 'neg_mean_squared_error',
    'rmse': 'neg_root_mean_squared_error',
    'mae': 'neg_mean_absolute_error',
}

# Run cross-validation
cv_results = cross_validate(linear_model, X_train, y_train, cv=kf, scoring=scoring)

# Calculate averages
lr_avg_r2 = np.mean(cv_results['test_r2']) * 100
lr_avg_mse = -np.mean(cv_results['test_mse'])
lr_avg_rmse = -np.mean(cv_results['test_rmse'])
lr_avg_mae = -np.mean(cv_results['test_mae'])

# Calculate standard deviations
lr_std_r2 = np.std(cv_results['test_r2']) * 100
lr_std_mse = np.std(-cv_results['test_mse'])
lr_std_rmse = np.std(-cv_results['test_rmse'])
lr_std_mae = np.std(-cv_results['test_mae'])

# Print results
print("Linear Regression Model Cross Validation:")
print(f"R-squared (R²): {lr_avg_r2:.2f}% ± {lr_std_r2:.2f}%")
print(f"MSE: {lr_avg_mse:.4f} ± {lr_std_mse:.4f}")
print(f"RMSE: {lr_avg_rmse:.4f} ± {lr_std_rmse:.4f}")
print(f"MAE: {lr_avg_mae:.4f} ± {lr_std_mae:.4f}")

"""### 5.2.2 Decision Tree Model"""

# Perform cross-validation using the correct model variable
cv_results = cross_validate(tree_model, X_train, y_train, cv=kf, scoring=scoring)

# Calculate average scores
dt_avg_r2 = np.mean(cv_results['test_r2']) * 100
dt_avg_mse = -np.mean(cv_results['test_mse'])
dt_avg_rmse = -np.mean(cv_results['test_rmse'])
dt_avg_mae = -np.mean(cv_results['test_mae'])

# Calculate standard deviations
dt_std_r2 = np.std(cv_results['test_r2']) * 100
dt_std_mse = np.std(-cv_results['test_mse'])
dt_std_rmse = np.std(-cv_results['test_rmse'])
dt_std_mae = np.std(-cv_results['test_mae'])

# Print results
print("Decision Tree Model Cross Validation:")
print(f"R-squared (R²): {dt_avg_r2:.2f}% ± {dt_std_r2:.2f}%")
print(f"MSE: {dt_avg_mse:.4f} ± {dt_std_mse:.4f}")
print(f"RMSE: {dt_avg_rmse:.4f} ± {dt_std_rmse:.4f}")
print(f"MAE: {dt_avg_mae:.4f} ± {dt_std_mae:.4f}")

"""### 5.2.3 Random Forest Model"""

# Perform cross-validation
cv_results = cross_validate(forest_model, X_train, y_train, cv=kf, scoring=scoring)

# Calculate average metrics
rf_avg_r2 = np.mean(cv_results['test_r2']) * 100
rf_avg_mse = -np.mean(cv_results['test_mse'])
rf_avg_rmse = -np.mean(cv_results['test_rmse'])
rf_avg_mae = -np.mean(cv_results['test_mae'])

# Calculate standard deviations
rf_std_r2 = np.std(cv_results['test_r2']) * 100
rf_std_mse = np.std(-cv_results['test_mse'])
rf_std_rmse = np.std(-cv_results['test_rmse'])
rf_std_mae = np.std(-cv_results['test_mae'])

# Print results
print("Random Forest Model Cross Validation:")
print(f"R-squared (R²): {rf_avg_r2:.2f}% ± {rf_std_r2:.2f}%")
print(f"MSE: {rf_avg_mse:.4f} ± {rf_std_mse:.4f}")
print(f"RMSE: {rf_avg_rmse:.4f} ± {rf_std_rmse:.4f}")
print(f"MAE: {rf_avg_mae:.4f} ± {rf_std_mae:.4f}")

"""### 5.2.4 KNN Model"""

# Perform cross-validation
cv_results = cross_validate(knn_model, X_train, y_train, cv=kf, scoring=scoring)

# Calculate average metrics
knn_avg_r2 = np.mean(cv_results['test_r2']) * 100
knn_avg_mse = -np.mean(cv_results['test_mse'])
knn_avg_rmse = -np.mean(cv_results['test_rmse'])
knn_avg_mae = -np.mean(cv_results['test_mae'])

# Calculate standard deviations
knn_std_r2 = np.std(cv_results['test_r2']) * 100
knn_std_mse = np.std(-cv_results['test_mse'])
knn_std_rmse = np.std(-cv_results['test_rmse'])
knn_std_mae = np.std(-cv_results['test_mae'])

# Print results
print("K-Nearest Neighbors Model Cross Validation:")
print(f"R-squared (R²): {knn_avg_r2:.2f}% ± {knn_std_r2:.2f}%")
print(f"MSE: {knn_avg_mse:.4f} ± {knn_std_mse:.4f}")
print(f"RMSE: {knn_avg_rmse:.4f} ± {knn_std_rmse:.4f}")
print(f"MAE: {knn_avg_mae:.4f} ± {knn_std_mae:.4f}")